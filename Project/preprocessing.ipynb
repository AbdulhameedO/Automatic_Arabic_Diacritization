{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tashkeel </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is developed by Abdulhameed Osama, Hossam Nabil and Nourhan Mohamed as a part of the Natural Language Processing course (NLP) at Cairo University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a Bi-LSTM model is trained to predict the diacritics of Arabic text. The model is trained on a dataset of 18 million characters from various domains. The model is trained on Google Colab using a Tesla K80 GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieved an accuracy of 96.5% on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model starts with a character embedding layer, followed by a Bi-LSTM layer, then a dense layer and finally a softmax layer. The model is trained using the Adam optimizer and the categorical crossentropy loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Imports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import helper_file as hf\n",
    "\n",
    "# Arabic Text Preprocessing\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "# import pyarabic.number as number\n",
    "# from pyarabic.araby import strip_tashkeel, strip_tatweel, normalize_ligature\n",
    "# from pyarabic.araby import tokenize, is_arabicrange, is_arabicword\n",
    "\n",
    "# Arabic Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arabic Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "dataset = hf.read_file('dataset/train.txt')\n",
    "\n",
    "arabic_letters = hf.read_pickle('Delivery/arabic_letters.pickle')\n",
    "\n",
    "\n",
    "diacritics = hf.read_pickle('Delivery/diacritics.pickle')\n",
    "\n",
    "diacritics_to_id = hf.read_pickle('Delivery/diacritics2id.pickle')\n",
    "print(diacritics_to_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset using bert tokenizer\n",
    "# tokenized_text = tokenizer.tokenize(dataset)\n",
    "# hf.write_pickle('tokenized_text.pickle', tokenized_text)\n",
    "\n",
    "tokenized_sentences = sent_tokenize(dataset)\n",
    "hf.write_pickle('tokenized_text_nltk.pickle', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = hf.read_pickle('tokenized_text.pickle')  # bert tokenized text\n",
    "nltk = hf.read_pickle('tokenized_text_nltk.pickle')  # nltk tokenized text\n",
    "\n",
    "print(bert[:1000])\n",
    "print(nltk[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# Clean the original dataset\n",
    "cleaned_dataset_with_diacritics = hf.clean_dataset(dataset, remove_diacritics=False)\n",
    "cleaned_dataset = hf.clean_dataset(dataset, remove_diacritics=True)\n",
    "hf.write_file('Delivery/cleaned_dataset.txt', cleaned_dataset)\n",
    "hf.write_file('Delivery/cleaned_dataset_with_diacritics.txt', cleaned_dataset_with_diacritics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_arabic_sentences(text):\n",
    "   \n",
    "    pattern = r'(?<=[.؟!,؛])'\n",
    "\n",
    "    # Split the text into sentences based on the pattern\n",
    "    sentences = re.split(pattern, text)\n",
    "    # remove punctuations\n",
    "    sentences = [re.sub(r'[.؟!،؛]', '', sent) for sent in sentences]\n",
    "    sentences = [re.sub(r'\\s+', ' ', sent) for sent in sentences]\n",
    "    sentences = filter(lambda sentences: sentences.strip(), sentences)\n",
    "\n",
    "    return sentences \n",
    "\n",
    "\n",
    "def append_to_file(file_path, content):\n",
    "    with open(file_path, 'a', encoding='utf-8') as file:\n",
    "        file.write(content.strip() + '\\n')\n",
    "\n",
    "\n",
    "sentences = split_arabic_sentences(cleaned_dataset)\n",
    "\n",
    "\n",
    "for s in sentences:\n",
    "    append_to_file('Delivery/sentences.txt', s)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [187], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m diacritized_sentences \u001b[38;5;241m=\u001b[39m \u001b[43msplit_arabic_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_dataset_with_diacritics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m diacritized_sentences:\n\u001b[0;32m      5\u001b[0m     append_to_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDelivery/diacritized_sentences.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, s)\n",
      "Cell \u001b[1;32mIn [171], line 6\u001b[0m, in \u001b[0;36msplit_arabic_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=[.؟!,؛])\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Split the text into sentences based on the pattern\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m sentences \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# remove punctuations\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[.؟!،؛]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, sent) \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "diacritized_sentences = split_arabic_sentences(cleaned_dataset_with_diacritics)\n",
    "for s in diacritized_sentences:\n",
    "    append_to_file('Delivery/diacritized_sentences.txt', s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 28, 34, 30, 33, 0, 30, 18, 31, 10, 33, 0, 30, 31, 7, 0, 28, 30, 32, 7, 0, 36, 17, 36, 15, 0, 28, 34, 30, 33]\n",
      "[0, 0, 6, 2, 2, 15, 0, 4, 0, 6, 2, 15, 4, 0, 14, 15, 2, 6, 0, 14, 15, 2, 4, 14, 2, 15, 0, 6, 0, 2]\n",
      "30\n",
      "30\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pyarabic import araby\n",
    "\n",
    "\n",
    "def get_diacritics(text,chars,diacritics2id):\n",
    "    text = list(text)\n",
    "    string = ''\n",
    "    diacritics = []\n",
    "    counter = 0 \n",
    "    for char in text:\n",
    "        if char == ' ':\n",
    "            # print('space ////////////////////////////////////')\n",
    "            # print(f'{string} appeneed to diacritics list space condition')\n",
    "            diacritics.append(diacritics_to_id[string])\n",
    "            diacritics.append(15)\n",
    "            string = ''\n",
    "            counter += 1\n",
    "            continue\n",
    "    \n",
    "        if char not in chars:\n",
    "            # print(f\"diacritic {char}\")\n",
    "            string += char\n",
    "            # print('concatenated string',string)\n",
    "        else:\n",
    "            # print('regular char ',char)\n",
    "            if text[counter - 1] in chars:\n",
    "                # print(f'{string} appeneed to diacritics list counter condition')\n",
    "                diacritics.append(diacritics2id[string])\n",
    "                string = ''\n",
    "            elif string != '':\n",
    "                # print(f'{string} appeneed to diacritics list normal condidition')\n",
    "                diacritics.append(diacritics2id[string])\n",
    "    \n",
    "            string = ''\n",
    "\n",
    "        counter += 1    \n",
    "\n",
    "    return diacritics\n",
    "\n",
    "def get_tashkeel(text,char,diacritics2id):\n",
    "    diacritics = get_diacritics(text,char,diacritics2id)\n",
    "    if text[-1] not in char:\n",
    "        diacritics.append(diacritics2id[text[-1]])\n",
    "    return diacritics\n",
    "\n",
    "        \n",
    "            \n",
    "sen = \"وقوله لزمته لما قلنا يريد قوله\" \n",
    "sen = [char_to_idx[char] for char in sen]\n",
    "\n",
    "undiacritized_sentences = [\"محمد\", \"جمال\", \"علي\"]\n",
    "\n",
    "\n",
    "\n",
    "chars = sorted(arabic_letters) \n",
    "char_to_idx = {char: idx + 1 for idx, char in enumerate(chars)}  # Assigning 0 for padding\n",
    "char_to_idx[' '] = 0\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "idx_to_char[0] = ' ' \n",
    "undiacritized_sequences = [[char_to_idx[char] for char in sentence] for sentence in undiacritized_sentences]\n",
    "# diacritic_labels = [get_diacritics(text,chars,diacritics_to_id) for text in diacritized_sentences]\n",
    "diacritic_labels = get_tashkeel(\"وَقَوْلُهُ لَزِمَتْهُ لِمَا قُلْنَا يُرِيدُ قَوْلَهُ\",chars,diacritics_to_id)\n",
    "print(sen)\n",
    "print(diacritic_labels)\n",
    "print(len(sen))\n",
    "print(len(diacritic_labels))\n",
    "print(len(chars))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TashkeelDataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, x, y, pad):\n",
    "    \"\"\"\n",
    "    This is the constructor of the NERDataset\n",
    "    Inputs:\n",
    "    - x: a list of lists where each list contains the ids of the tokens\n",
    "    - y: a list of lists where each list contains the label of each token in the sentence\n",
    "    - pad: the id of the <PAD> token (to be used for padding all sentences and labels to have the same length)\n",
    "    \"\"\"\n",
    "    ##################### TODO: create two tensors one for x and the other for labels ###############################\n",
    "    max_size = max([len(i) for i in x]) # find the max length of the sentences \n",
    "    print('The max size is', max_size)  \n",
    "    self.x = torch.tensor([i + [pad] * (max_size - len(i)) for i in x]) # pad the sentences with <PAD> token\n",
    "    self.y = torch.tensor([i + [16] * (max_size - len(i)) for i in y]) # pad the labels with <PAD> token\n",
    "    #################################################################################################################\n",
    "\n",
    "  def __len__(self):\n",
    "    \"\"\"\n",
    "    This function should return the length of the dataset (the number of sentences)\n",
    "    \"\"\"\n",
    "    ###################### TODO: return the length of the dataset #############################\n",
    "    return len(self.x)\n",
    "\n",
    "    ###########################################################################################\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    \"\"\"\n",
    "    This function returns a subset of the whole dataset\n",
    "    \"\"\"\n",
    "    ###################### TODO: return a tuple of x and y ###################################\n",
    "    return (self.x[idx], self.y[idx])\n",
    "    ##########################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gomal = hf.read_file('Delivery/sentences.txt')\n",
    "gomal_tashkeel = hf.read_file('Delivery/diacritized_sentences.txt')\n",
    "\n",
    "x = []\n",
    "for line in gomal.splitlines():\n",
    "    x.append([char_to_idx[char] for char in line])\n",
    "\n",
    "y = []\n",
    "for line in gomal_tashkeel.splitlines():\n",
    "    y.append(get_tashkeel(line,chars,diacritics_to_id))\n",
    "\n",
    "print(len(x[0]))\n",
    "print(len(y[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "425\n",
      "425\n",
      "1\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "max_label_size = max([len(i) for i in y[0:8]])\n",
    "max_x_size = max([len(i) for i in x[0:8]])\n",
    "print(max_label_size)\n",
    "print(max_x_size)\n",
    "for i in range(0,8):\n",
    "    if len(y[i]) == max_label_size:\n",
    "        print(i)\n",
    "        \n",
    "print(len(diacritics_to_id))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "mini_sentences = x[0: 8]\n",
    "mini_labels = y[0: 8]\n",
    "mini_dataset = TashkeelDataset(mini_sentences, mini_labels, 40)\n",
    "dummy_dataloader = torch.utils.data.DataLoader(mini_dataset, batch_size=5)\n",
    "dg = iter(dummy_dataloader)\n",
    "X1, Y1 = next(dg)\n",
    "X2, Y2 = next(dg)\n",
    "print(Y1.shape, X1.shape, Y2.shape, X2.shape)\n",
    "print(X1[0][:], \"\\n\", Y1[0][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tashkeel(nn.Module):\n",
    "  def __init__(self, vocab_size=37, embedding_dim=37, hidden_size=50, n_classes=16):\n",
    "    \"\"\"\n",
    "    character level tashkeel model\n",
    "    The constructor of our NER model\n",
    "    Inputs:\n",
    "    - vocab_size: the number of unique characters in the dataset\n",
    "    - embedding_dim: the embedding dimension\n",
    "    - n_classes: the number of final classes (tags)\n",
    "    \"\"\"\n",
    "    super(Tashkeel, self).__init__()\n",
    "    ####################### TODO: Create the layers of your model #######################################\n",
    "    # (1) Create the embedding layer\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    # (2) Create an LSTM layer with hidden size = hidden_size and batch_first = True\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True,bidirectional=True)\n",
    "\n",
    "    # (3) Create a linear layer with number of neorons = n_classes\n",
    "    self.linear = nn.Linear(hidden_size, n_classes)\n",
    "    #####################################################################################################\n",
    "\n",
    "  def forward(self, sentences):\n",
    "    \"\"\"\n",
    "    This function does the forward pass of our model\n",
    "    Inputs:\n",
    "    - sentences: tensor of shape (batch_size, max_length)\n",
    "\n",
    "    Returns:\n",
    "    - final_output: tensor of shape (batch_size, max_length, n_classes)\n",
    "    \"\"\"\n",
    "\n",
    "    final_output = None\n",
    "    ######################### TODO: implement the forward pass ####################################\n",
    "    # (1) pass the sentences through the embedding layer to get the embeddings\n",
    "    final_output = self.embedding(sentences)\n",
    "    final_output, _ = self.lstm(final_output)\n",
    "    final_output = self.linear(final_output)\n",
    "    # print(_)\n",
    "    ###############################################################################################\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tashkeel(\n",
      "  (embedding): Embedding(37, 37)\n",
      "  (lstm): LSTM(37, 50, batch_first=True, bidirectional=True)\n",
      "  (linear): Linear(in_features=50, out_features=16, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Tashkeel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset, batch_size=10, epochs=5, learning_rate=0.01):\n",
    "  \"\"\"\n",
    "  This function implements the training logic\n",
    "  Inputs:\n",
    "  - model: the model ot be trained\n",
    "  - train_dataset: the training set of type NERDataset\n",
    "  - batch_size: integer represents the number of examples per step\n",
    "  - epochs: integer represents the total number of epochs (full training pass)\n",
    "  - learning_rate: the learning rate to be used by the optimizer\n",
    "  \"\"\"\n",
    "\n",
    "  ############################## TODO: replace the Nones in the following code ##################################\n",
    "  \n",
    "  # (1) create the dataloader of the training set (make the shuffle=True)\n",
    "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "\n",
    "  # (2) make the criterion cross entropy loss\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  # (3) create the optimizer (Adam)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # GPU configuration\n",
    "  use_cuda = torch.cuda.is_available()\n",
    "  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "  if use_cuda:\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "  for epoch_num in range(epochs):\n",
    "    total_acc_train = 0\n",
    "    total_loss_train = 0\n",
    "\n",
    "    for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "      # (4) move the train input to the device\n",
    "      train_label = train_label.to(device)\n",
    "\n",
    "      # (5) move the train label to the device\n",
    "      train_input = train_input.to(device)\n",
    "\n",
    "\n",
    "      # (6) do the forward pass\n",
    "      output = model(train_input)\n",
    "      \n",
    "      # (7) loss calculation (you need to think in this part how to calculate the loss correctly)\n",
    "      batch_loss = criterion(output.view(-1, 16), train_label.view(-1))\n",
    "\n",
    "      # (8) append the batch loss to the total_loss_train\n",
    "      total_loss_train += batch_loss.item()\n",
    "      \n",
    "      # (9) calculate the batch accuracy (just add the number of correct predictions)\n",
    "      acc = (output.argmax(dim=2) == train_label).sum().item()\n",
    "      total_acc_train += acc\n",
    "\n",
    "      # (10) zero your gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # (11) do the backward pass\n",
    "      batch_loss.backward()\n",
    "\n",
    "      # (12) update the weights with your optimizer\n",
    "      optimizer.step()\n",
    "      \n",
    "    # epoch loss\n",
    "    epoch_loss = total_loss_train / len(train_dataset)\n",
    "\n",
    "    # (13) calculate the accuracy\n",
    "    epoch_acc = total_acc_train / (len(train_dataset) * len(train_dataset[0][0]))\n",
    "    print(\n",
    "        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n",
    "        | Train Accuracy: {epoch_acc}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max size is 671\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TashkeelDataset(x[0:20], y[0:20], 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [190], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [189], line 45\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     41\u001b[0m train_input \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# (6) do the forward pass\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# (7) loss calculation (you need to think in this part how to calculate the loss correctly)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m), train_label\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [179], line 36\u001b[0m, in \u001b[0;36mTashkeel.forward\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m     33\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m######################### TODO: implement the forward pass ####################################\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# (1) pass the sentences through the embedding layer to get the embeddings\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m final_output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(final_output)\n\u001b[0;32m     38\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(final_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "train(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

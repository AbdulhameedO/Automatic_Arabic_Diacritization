{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset using bert tokenizer\n",
    "tokenized_text = tokenizer.tokenize(dataset)\n",
    "hf.write_pickle('tokenized_text.pickle', tokenized_text)\n",
    "\n",
    "tokenized_sentences = sent_tokenize(dataset)\n",
    "hf.write_pickle('tokenized_text_nltk.pickle', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, test_dataset, batch_size=5):\n",
    "#   \"\"\"\n",
    "#   This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "#   Inputs:\n",
    "#   - model: a NER model\n",
    "#   - test_dataset: dataset of type NERDataset\n",
    "#   \"\"\"\n",
    "#   ########################### TODO: Replace the Nones in the following code ##########################\n",
    "\n",
    "#   # (1) create the test data loader\n",
    "#   test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "#   # GPU Configuration\n",
    "#   use_cuda = torch.cuda.is_available()\n",
    "#   device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#   if use_cuda:\n",
    "#     model = model.cuda()\n",
    "\n",
    "#   total_acc_test = 0\n",
    "\n",
    "#   # (2) disable gradients\n",
    "#   with torch.no_grad():\n",
    "\n",
    "#     for test_input, test_label in tqdm(test_dataloader):\n",
    "#       # (3) move the test input to the device\n",
    "#       test_label = test_label.to(device)\n",
    "\n",
    "#       # (4) move the test label to the device\n",
    "#       test_input = test_input.to(device)\n",
    "\n",
    "#       # (5) do the forward pass\n",
    "#       output = model(test_input)\n",
    "\n",
    "#       # accuracy calculation (just add the correct predicted items to total_acc_test)\n",
    "#       acc = (output.argmax(dim=2) == test_label).sum().item()\n",
    "#       total_acc_test += acc\n",
    "\n",
    "#     # (6) calculate the over all accuracy\n",
    "#     total_acc_test /= (len(test_dataset) * len(test_dataset[0][0]))\n",
    "#   ##################################################################################################\n",
    "\n",
    "\n",
    "#   print(f'\\nTest Accuracy: {total_acc_test}')\n",
    "\n",
    "def evaluate(model, test_dataset, batch_size=5):\n",
    "    \"\"\"\n",
    "    This function takes a NER model and evaluates its performance (accuracy) on a test data\n",
    "    Inputs:\n",
    "    - model: a NER model\n",
    "    - test_dataset: dataset of type NERDataset\n",
    "    \"\"\"\n",
    "    # Create the test data loader\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # GPU Configuration\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # Disable gradients\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in tqdm(test_dataloader):\n",
    "            test_label = test_label.to(device)\n",
    "            test_input = test_input.to(device)\n",
    "            output = model(test_input)\n",
    "\n",
    "            predicted_labels = output.argmax(dim=2)\n",
    "            correct_predictions = ((predicted_labels == test_label) & (test_label != 15)).sum().item()\n",
    "            total_correct_predictions += correct_predictions\n",
    "\n",
    "            valid_predictions = (test_label != 15).sum().item()\n",
    "            total_predictions += valid_predictions\n",
    "            # space_pred = (predicted_labels == 14).sum().item()\n",
    "            # total_correct_predictions +=space_pred\n",
    "            # valid_predictions = (test_label == 14).sum().item()\n",
    "            # total_predictions += valid_predictions\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the overall accuracy excluding label 15\n",
    "    overall_accuracy = total_correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "\n",
    "    print(f'\\nTest Accuracy (excluding label 15): {overall_accuracy}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    evaluate(model, val_dataset)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
